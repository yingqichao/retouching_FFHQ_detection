#### models path dirs
focal_alpha: 0.5
network_arch: shunt_convnext #convnextv1 #shunt_convnext, conformer, efficient

## checkpoint stack
model_load_number: shunt_conv_dual_512_ablate_4times_tokens/Epoch2_131999_ViT.pth
#### general settings
conduct_train: false
conduct_test: true
test_save_checkpoint: true
gpu_ids: [0]
operations_list: ['none','single','dual','three','four']
task_name: shunt_conv_dual_512_ablate_4times_tokens

restart_step: 1000
model_save_period: 2000

with_dual_process: false

drop_tokens_per_layer: 16 # 32 or 64


datasets:
  train:
    name: Rerun
    mode: LQGT
#    dataroot_GT: /qichaoying/Downloads/Invertible-Image-Rescaling-master/icassp_real/images    # Test images
#    dataroot_GT: /groupshare//Places/data_256                                        # Places
#    dataroot_GT: /groupshare/CelebA/img/img_celeba_200                                          # CelebA
#    dataroot_GT: /groupshare/paris_street_view/paris_train                            # ParisStreetView
    dataroot_GT: /groupshare/ISP_results/xxhu_test/UNet/FORGERY_0                                   # COCO
#    dataroot_GT: /groupshare//Flickr1024/train                                 # Flicker
#    dataroot_GT: /groupshare//DIV2K_valid                                # DIV2K
#    dataroot_GT: /groupshare//VOC2012/JPEGImages
#    dataroot_GT: /groupshare//UCID_color/images                                                 # UCID
#    dataroot_GT: /groupshare//ILSVRC2012_img_val                                      # ImageNet
    dataroot_LQ: /groupshare/ISP_results/xxhu_test/UNet/MASK

    use_shuffle: true
    n_workers: 2 # per GPU

    batch_size: 8 # 32 for hierarchical
    GT_size: 512 # 608

    color: RGB

  val:
    name: val_DIV2K
    mode: LQGT
    dataroot_GT: /groupshare/ISP_results/xxhu_test/UNet/FORGERY_0
    dataroot_LQ: /groupshare/ISP_results/xxhu_test/UNet/MASK # path to validation reference LR images, not necessary, if not provided, LR images will be generated in dataloader
    GT_size: 512


train:
  lr_CNN: !!float 1e-4
  lr_transformer: !!float 5e-5
  beta1: 0.9 #0.9
  beta2: 0.999 #0.999
